# ğŸ—„ï¸ Online kurz Big Data (VeÄ¾kÃ© DÃ¡ta), Apache Spark, Hive, Apache Hadoop, Apache Airflow

> PraktickÃ© kurzy â€“ RDD, DataFrame, SparkSQL a distribuovanÃ© spracovanie dÃ¡t

## ğŸ“˜ Obsah kurzu
01. [**ğŸ” Ãšvod do veÄ¾kÃ½ch dÃ¡t a Apache Spark**](#uvod-spark)
02. [**ğŸ§± PrÃ¡ca s RDD a DataFrame**](#rdd-dataframe)
03. [**ğŸ§  Spark SQL a dopyty nad dÃ¡tami**](#spark-sql)
04. [**âš™ï¸ Nastavenie prostredia a Spark UI**](#nastavenie)
05. [**ğŸ“Š NaÄÃ­tanie dÃ¡t a transformÃ¡cie**](#transformacie)
06. [**ğŸ“š Zdroje a odporÃºÄania pre Apache Spark**](#zdroje)

---
<a name="uvod-spark"></a>
# ğŸ” 1. Ãšvod do veÄ¾kÃ½ch dÃ¡t a nÃ¡stroje pre spracovanie veÄ¾kÃ½ch dÃ¡t (Apache Spark, Hive, Apache Hadoop, Apache Airflow)

- **Big Data** rieÅ¡ia spracovanie dÃ¡t, ktorÃ© sÃº prÃ­liÅ¡ veÄ¾kÃ©, rÃ½chle alebo rÃ´znorodÃ© pre klasickÃ© databÃ¡zy.
- **Apache Spark** je vÃ½konnÃ½ engine na rÃ½chle spracovanie.
- **Hive** umoÅ¾Åˆuje analytikom pouÅ¾Ã­vaÅ¥ SQL nad dÃ¡tami v HDFS.
- **Hadoop** poskytuje zÃ¡kladnÃº infraÅ¡truktÃºru pre ukladanie a dÃ¡vkovÃ© vÃ½poÄty.
- **Airflow** je urÄenÃ½ na plÃ¡novanie a orchestrÃ¡ciu dÃ¡tovÃ½ch procesov.

## ğŸ“Š ÄŒo sÃº veÄ¾kÃ© dÃ¡ta â€“ model 12V a na Äo sÃº dobrÃ©?

**Big Data** oznaÄuje veÄ¾kÃ© objemy dÃ¡t, ktorÃ© sa vyznaÄujÃº vysokou **rÃ½chlosÅ¥ou**, **objemom**, **rÃ´znorodosÅ¥ou** a Äasto aj **nÃ­zkou kvalitou**. VeÄ¾kÃ© dÃ¡ta sa v sÃºÄasnosti nedefinujÃº uÅ¾ len cez zÃ¡kladnÃ© 3 alebo 5 znakov, ale cez **12 dimenziÃ­ (12V)**, ktorÃ© lepÅ¡ie vystihujÃº komplexnosÅ¥ ich spracovania, hodnoty a rizÃ­k.

| ğŸ†” **VlastnosÅ¥** | ğŸ“Œ **Popis**                                 | ğŸ’¡ **PrÃ­klad**                                |
|------------------|----------------------------------------------|-----------------------------------------------|
| ğŸ“¦ Volume         | Objem dÃ¡t â€“ terabajty aÅ¾ petabajty           | TransakÄnÃ© dÃ¡ta, zÃ¡znamy zo senzorov          |
| âš¡ Velocity       | RÃ½chlosÅ¥ generovania dÃ¡t                     | DÃ¡tovÃ© toky z IoT zariadenÃ­, streamy videa    |
| ğŸ§© Variety        | RÃ´znorodosÅ¥ dÃ¡t â€“ Å¡truktÃºrovanÃ© aj neÅ¡trukt. | CSV, JSON, obrÃ¡zky, logy, XML                 |
| âœ… Veracity       | VierohodnosÅ¥ a kvalita dÃ¡t                   | ChÃ½bajÃºce hodnoty, nekonzistentnÃ© zÃ¡znamy     |
| ğŸ’° Value          | Hodnota, ktorÃº je moÅ¾nÃ© z dÃ¡t zÃ­skaÅ¥         | AnalÃ½zy zÃ¡kaznÃ­kov, predikcie, odporÃºÄania    |
| ğŸ” Variability    | Zmena v Å¡truktÃºre alebo rÃ½chlosti                | SezÃ³nne vÃ½kyvy v dÃ¡tach, nÃ¡razovÃ© zÃ¡Å¥aÅ¾e       |
| ğŸ§  Visualization   | Potreba zrozumiteÄ¾nÃ©ho znÃ¡zornenia               | Grafy, dashboardy, heatmapy                    |
| ğŸ•µï¸â€â™‚ï¸ Validity      | RelevantnosÅ¥ a konzistentnosÅ¥ v Äase            | AktuÃ¡lne vs. historickÃ© dÃ¡ta, verzovanie       |
| ğŸ§± Volatility     | TrvÃ¡cnosÅ¥ a Å¾ivotnosÅ¥ dÃ¡t                        | KrÃ¡tkodobÃ© (cache) vs. dlhodobÃ© ukladanie      |
| ğŸ” Vulnerability  | RizikovosÅ¥ a citlivosÅ¥ na bezpeÄnosÅ¥             | OsobnÃ© Ãºdaje, GDPR, anonymizÃ¡cia               |
| ğŸ”„ Variance       | Rozdiely v dÃ¡tach pri rovnakÃ½ch vstupoch         | RÃ´zne senzory dÃ¡vajÃº inÃ© hodnoty               |
| ğŸ¯ Venue          | Miesto pÃ´vodu, kontext a zdroj dÃ¡t               | MobilnÃ© zariadenia, cloud, edge, on-prem       |

### ğŸ¯ Na Äo sa Big Data pouÅ¾Ã­vajÃº?

- AnalÃ½za sprÃ¡vania zÃ¡kaznÃ­kov (marketing, e-commerce)
- Detekcia podvodov (banky, poistenie)
- Predikcia dopytu a spotreby (vÃ½roba, logistika)
- Real-time monitoring (IoT, zdravotnÃ­ctvo)
- AutomatizovanÃ© rozhodovanie (AI, ML)

## âš™ï¸ ÄŒo je to Apache Spark a na Äo je dobrÃ½?

**Apache Spark** je distribuovanÃ½ engine pre spracovanie veÄ¾kÃ½ch dÃ¡t v pamÃ¤ti (in-memory). Apache Spark je vÃ½konnÃ½ open-source engine na spracovanie veÄ¾kÃ½ch dÃ¡t v reÃ¡lnom Äase. Podporuje paralelnÃ© vÃ½poÄty v pamÃ¤ti a je Å¡iroko pouÅ¾Ã­vanÃ½ v oblasti dÃ¡tovej analytiky, strojovÃ©ho uÄenia a streamovania. Je vÃ½konnÃ½, Å¡kÃ¡lovateÄ¾nÃ½ a flexibilnÃ½ nÃ¡stroj pre spracovanie veÄ¾kÃ½ch dÃ¡t. Je distribuovanÃ½ pod **licenciou Apache 2.0**

### âœ… Na Äo sa pouÅ¾Ã­va?
- RÃ½chle dÃ¡vkovÃ© a interaktÃ­vne spracovanie
- StrojovÃ© uÄenie (MLlib)
- PrÃ¡ca s DataFrame a SQL
- Streamovanie dÃ¡t v reÃ¡lnom Äase

## ğŸ ÄŒo je to Apache Hive a na Äo je dobrÃ½?

**Apache Hive** je SQL-like vrstva nad veÄ¾kÃ½mi dÃ¡tami uloÅ¾enÃ½mi v HDFS alebo inÃ½ch formÃ¡toch.

### âœ… Na Äo sa pouÅ¾Ã­va?
- Dopytovanie nad dÃ¡tami pomocou SQL
- VytvÃ¡ranie tabuliek, ETL procesy
- IntegrÃ¡cia s Hadoopom a Spark SQL
- Reporting a analÃ½zy nad Å¡truktÃºrovanÃ½mi dÃ¡tami


## ğŸ˜ ÄŒo je to Apache Hadoop a na Äo je dobrÃ½?
**Apache Hadoop** je ekosystÃ©m pre distribuovanÃ© ukladanie a spracovanie veÄ¾kÃ½ch dÃ¡t.

### âœ… Na Äo sa pouÅ¾Ã­va?
- Ukladanie dÃ¡t pomocou **HDFS** (Hadoop Distributed File System)
- Spracovanie pomocou **MapReduce**
- VyuÅ¾Ã­va sa ako zÃ¡kladnÃ¡ vrstva pre Spark, Hive, HBase
- UmoÅ¾Åˆuje horizontÃ¡lne Å¡kÃ¡lovanie (viac serverov)


## ğŸ§­ ÄŒo je to Apache Airflow a na Äo je dobrÃ½?

**Apache Airflow** je nÃ¡stroj na plÃ¡novanie a riadenie dÃ¡tovÃ½ch workflowov (DAG â€“ Directed Acyclic Graphs).

### âœ… Na Äo sa pouÅ¾Ã­va?
- AutomatizÃ¡cia ETL/ELT procesov
- PlÃ¡novanie Spark/Hive/Hadoop Ãºloh
- Riadenie zÃ¡vislostÃ­ medzi Ãºlohami
- VizualizÃ¡cia a monitoring workflowov


## ğŸ“Š PorovnÃ¡vacia tabuÄ¾ka: Apache nÃ¡stroje pre Big Data

| NÃ¡stroj         | HlavnÃ© vyuÅ¾itie                       | TechnolÃ³gia               | VÃ½hody                                 | NevÃ½hody                                 |
|------------------|----------------------------------------|----------------------------|----------------------------------------|------------------------------------------|
| **Apache Spark** | RÃ½chle vÃ½poÄty, ML, stream, SQL       | In-memory distribÃºcia     | VÃ½kon, univerzÃ¡lnosÅ¥, Å¡kÃ¡lovateÄ¾nosÅ¥  | VyÅ¡Å¡ie nÃ¡roky na pamÃ¤Å¥                   |
| **Apache Hive**  | SQL nad veÄ¾kÃ½mi dÃ¡tami (HDFS)         | SQL-like nad Hadoop       | ZnÃ¡ma syntax, vhodnÃ© na reporty        | PomalÅ¡ie, nie real-time                  |
| **Apache Hadoop**| Ukladanie a dÃ¡vkovÃ© spracovanie       | HDFS + MapReduce          | RobustnÃ©, osvedÄenÃ© rieÅ¡enie           | StarÅ¡ie, pomalÅ¡ie ako Spark              |
| **Apache Airflow**| Riadenie workflowov a plÃ¡novanie     | Python, DAG workflow      | Modularita, monitoring, REST API       | VyÅ¡Å¡ia krivka uÄenia, komplexnÃ© ladenie  |


## âš™ï¸ PreÄo Apache Spark?

| ğŸ’¡ VlastnosÅ¥         | ğŸ”¥ Apache Spark                                      |
|----------------------|-----------------------------------------------------|
| ğŸ’¾ Spracovanie       | V pamÃ¤ti (in-memory), rÃ½chlejÅ¡ie ako Hadoop         |
| ğŸ§  ProgramovacÃ­ model| RDD, DataFrame, SQL, MLlib, GraphX                  |
| ğŸŒ Podpora jazykov   | Python, Scala, Java, R                              |
| ğŸ“ˆ VyuÅ¾itie          | Batch, stream, interaktÃ­vne, strojovÃ© uÄenie        |
| ğŸ“š EkosystÃ©m         | BohatÃ¡ dokumentÃ¡cia, rozÅ¡Ã­renia, kompatibilita     |


## ğŸ—ï¸ ArchitektÃºra Apache Spark

- **Driver Program** â€“ riadi vykonÃ¡vanie a vytvÃ¡ra DAG
- **Cluster Manager** â€“ prideÄ¾uje zdroje (napr. YARN, Kubernetes)
- **Executors** â€“ vykonÃ¡vajÃº Ãºlohy a spracÃºvajÃº dÃ¡ta
- **Tasks** â€“ jednotky paralelnÃ©ho vÃ½poÄtu

ğŸŒ€ **DAG (Directed Acyclic Graph)** â€“ reprezentuje logiku vÃ½poÄtu ako necyklickÃ½ graf zÃ¡vislostÃ­.


## ğŸ§± Moduly Apache Spark

| Modul            | Popis                                             |
|------------------|---------------------------------------------------|
| `Spark Core`     | ZÃ¡kladnÃ© API, sprÃ¡va pamÃ¤te a plÃ¡novanie vÃ½poÄtu |
| `Spark SQL`      | Dopytovanie cez SQL a DataFrame API              |
| `Spark MLlib`    | NÃ¡stroje pre strojovÃ© uÄenie                     |
| `Spark Streaming`| StreamovÃ© (real-time) spracovanie                |
| `GraphX`         | GrafovÃ© vÃ½poÄty a analÃ½zy                        |


## ğŸ“¦ PodporovanÃ© formÃ¡ty a zdroje dÃ¡t

- **FormÃ¡ty**: CSV, JSON, Parquet, Avro, ORC
- **Zdroje**: HDFS, S3, JDBC, Kafka, lokÃ¡lne sÃºbory, NoSQL databÃ¡zy


## ğŸ§  PrÃ­klady vyuÅ¾itia

| ğŸ¢ Odvetvie        | ğŸ“ˆ PrÃ­pad pouÅ¾itia                              |
|--------------------|--------------------------------------------------|
| FinTech             | Detekcia podvodov v reÃ¡lnom Äase                |
| E-commerce          | OdporÃºÄacie systÃ©my, personalizÃ¡cia ponÃºk       |
| ZdravotnÃ­ctvo       | Predikcia diagnÃ³z na zÃ¡klade historickÃ½ch dÃ¡t   |
| VÃ½roba / IoT        | PrediktÃ­vna ÃºdrÅ¾ba, sledovanie vÃ½konu strojov   |
| Marketing           | SegmentÃ¡cia zÃ¡kaznÃ­kov, analÃ½za sprÃ¡vania       |


## âœ… Zhrnutie

- Apache Spark je ideÃ¡lny nÃ¡stroj pre prÃ¡cu s veÄ¾kÃ½mi dÃ¡tami v rÃ´znych formÃ¡toch.
- PonÃºka vysokÃ½ vÃ½kon, Å¡kÃ¡lovateÄ¾nosÅ¥ a bohatÃ½ ekosystÃ©m nÃ¡strojov.
- Je vhodnÃ½ pre dÃ¡vkovÃ© aj real-time aplikÃ¡cie v mnohÃ½ch oblastiach.

---

<a name="rdd-dataframe"></a>
# ğŸ§± 2. PrÃ¡ca s RDD a DataFrame v Apache Spark

Apache Spark umoÅ¾Åˆuje dve hlavnÃ© abstrakcie pre prÃ¡cu s dÃ¡tami: **RDD (Resilient Distributed Dataset)** a **DataFrame**. V tejto kapitole si vysvetlÃ­me rozdiely, vÃ½hody a praktickÃ© prÃ­klady pouÅ¾itia oboch.


## ğŸ§  ÄŒo je RDD?

**RDD (Resilient Distributed Dataset)** je nÃ­zkoÃºrovÅˆovÃ¡, nemennÃ¡ kolekcia objektov, ktorÃ¡ sa distribuuje medzi uzly v klastri.

### âš™ï¸ Vlastnosti RDD:

- Immutability â€“ nemennÃ© Å¡truktÃºry
- ParalelnÃ© spracovanie
- Fault-tolerance â€“ automatickÃ¡ replikÃ¡cia a zotavenie
- Podpora funkcionÃ¡lnych operÃ¡ciÃ­ ako `map()`, `filter()`, `reduce()`

### ğŸ§ª PrÃ­klad: ZÃ¡kladnÃ¡ prÃ¡ca s RDD

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_squared = rdd.map(lambda x: x * x)
print(rdd_squared.collect())  # VÃ½stup: [1, 4, 9, 16, 25]
```

## ğŸ“„ TransformÃ¡cie a akcie na RDD

| Typ operÃ¡cie   | PrÃ­klad           | Popis                                       |
|----------------|-------------------|---------------------------------------------|
| TransformÃ¡cia  | `map()`, `filter()`| VytvÃ¡ra novÃ½ RDD                            |
| Akcia          | `collect()`, `count()` | SpustÃ­ vÃ½poÄet a vrÃ¡ti vÃ½sledok do drivera  |


## ğŸ“˜ ÄŒo je DataFrame?

**DataFrame** je vyÅ¡Å¡ia abstrakcia nad RDD s metadÃ¡tami (schema), podobnÃ¡ Pandas alebo SQL tabuÄ¾ke.

### ğŸ§¾ VÃ½hody DataFrame:

- OptimalizÃ¡cia pomocou Catalyst engine
- VÃ½razne rÃ½chlejÅ¡ie spracovanie ako RDD
- MoÅ¾nosÅ¥ pouÅ¾Ã­vaÅ¥ SQL-like syntax
- AutomatickÃ© spracovanie schÃ©my

### ğŸ§ª PrÃ­klad: Vytvorenie DataFrame

```python
from pyspark.sql import Row

df = spark.createDataFrame([Row(meno="Anna", vek=25), Row(meno="JÃ¡n", vek=32)])
df.show()
```


## ğŸ” BeÅ¾nÃ© operÃ¡cie s DataFrame

```python
df.filter(df.vek > 30).select("meno").show()
df.groupBy("vek").count().show()
```

| OperÃ¡cia          | Syntax                                     | Popis                           |
|-------------------|--------------------------------------------|----------------------------------|
| Filtrovanie       | `df.filter(df.vek > 30)`                   | VÃ½ber podÄ¾a podmienky           |
| VÃ½ber stÄºpcov     | `df.select("meno", "vek")`                 | VÃ½ber konkrÃ©tnych stÄºpcov       |
| AgregÃ¡cia         | `df.groupBy("vek").count()`                | SkupinovÃ© vÃ½poÄty               |
| Triedenie         | `df.orderBy("vek", ascending=False)`       | Zoradenie podÄ¾a hodnoty         |


## ğŸ” Porovnanie RDD vs. DataFrame

| VlastnosÅ¥             | RDD                                 | DataFrame                         |
|------------------------|--------------------------------------|------------------------------------|
| API Å¡tÃ½l              | FunkcionÃ¡lny (map, reduce)           | DeklaratÃ­vny (SQL-like)            |
| OptimalizÃ¡cia         | Bez optimalizÃ¡cie                    | Catalyst + Tungsten optimizÃ¡cia    |
| VÃ½kon                 | PomalÅ¡Ã­                              | RÃ½chlejÅ¡Ã­                          |
| ÄŒitateÄ¾nosÅ¥           | NiÅ¾Å¡ia (viac kÃ³du)                   | VyÅ¡Å¡ia (kompaktnejÅ¡Ã­ kÃ³d)          |
| PrÃ­stup k schÃ©me      | Nie                                  | Ãno                                |


## ğŸ”ƒ Prechod z RDD na DataFrame a spÃ¤Å¥

```python
# RDD â†’ DataFrame
from pyspark.sql import Row
rdd = spark.sparkContext.parallelize([Row(meno="Eva", vek=29)])
df = spark.createDataFrame(rdd)

# DataFrame â†’ RDD
rdd2 = df.rdd
```


## ğŸ§ª UkÃ¡Å¾ka prÃ¡ce s CSV sÃºborom ako DataFrame

```python
df_csv = spark.read.csv("data/osoby.csv", header=True, inferSchema=True)
df_csv.printSchema()
df_csv.select("meno", "vek").show()
```

## âœ… Zhrnutie

- **RDD** poskytuje nÃ­zkoÃºrovÅˆovÃº kontrolu nad dÃ¡tami, vhodnÃ© na zloÅ¾itÃ© transformÃ¡cie.
- **DataFrame** poskytuje vyÅ¡Å¡Ã­ vÃ½kon, ÄitateÄ¾nosÅ¥ a podporu SQL.
- V modernÃ½ch aplikÃ¡ciÃ¡ch sa odporÃºÄa pouÅ¾Ã­vaÅ¥ **DataFrame API**, ak nie je potrebnÃ© nieÄo Å¡pecifickÃ© z RDD.

---

<a name="spark-sql"></a>
# ğŸ§  3. Spark SQL a dopyty nad dÃ¡tami

Spark SQL je modul Apache Spark, ktorÃ½ umoÅ¾Åˆuje spracovanie Å¡truktÃºrovanÃ½ch dÃ¡t pomocou SQL syntaxe alebo DataFrame API. Kombinuje vÃ½konnosÅ¥ Spark enginu s jednoduchosÅ¥ou SQL.


## ğŸ“‹ ÄŒo je Spark SQL?

Spark SQL umoÅ¾Åˆuje:

- vykonÃ¡vaÅ¥ SQL dopyty priamo nad veÄ¾kÃ½mi dÃ¡tami,
- manipulovaÅ¥ so Å¡truktÃºrovanÃ½mi dÃ¡tami pomocou DataFrame API,
- pracovaÅ¥ s rÃ´znymi zdrojmi dÃ¡t ako CSV, Parquet, Hive, JDBC.


## ğŸ”§ Vytvorenie DataFrame tabuÄ¾ky
### ğŸ§ª PrÃ­klad: NaÄÃ­tanie CSV a registrÃ¡cia ako tabuÄ¾ka

```python
df = spark.read.option("header", "true").csv("data/objednavky.csv")
df.createOrReplaceTempView("objednavky")
```

Po registrÃ¡cii mÃ´Å¾ete nad `objednavky` spÃºÅ¡Å¥aÅ¥ SQL dopyty.


## ğŸ§ª PrÃ­klady SQL dopytov

```python
# VÃ½ber vÅ¡etkÃ½ch stÄºpcov
spark.sql("SELECT * FROM objednavky").show()

# Filtrovanie podÄ¾a hodnoty
spark.sql("SELECT * FROM objednavky WHERE cena > 100").show()

# SkupinovÃ© vÃ½poÄty
spark.sql("SELECT produkt, COUNT(*) AS pocet FROM objednavky GROUP BY produkt").show()
```

## ğŸ“Š Porovnanie: SQL vs. DataFrame API

| OperÃ¡cia                     | SQL syntax                                                    | DataFrame API                                 |
|------------------------------|----------------------------------------------------------------|-----------------------------------------------|
| VÃ½ber                        | `SELECT meno FROM zakaznici`                                  | `df.select("meno")`                           |
| Filtrovanie                  | `SELECT * FROM objednavky WHERE cena > 100`                   | `df.filter(df.cena > 100)`                    |
| AgregÃ¡cia                    | `SELECT AVG(cena) FROM objednavky`                            | `df.agg({"cena": "avg"})`                     |
| Zoskupenie                   | `SELECT produkt, COUNT(*) FROM objednavky GROUP BY produkt`   | `df.groupBy("produkt").count()`              |
| Triedenie                    | `SELECT * FROM objednavky ORDER BY datum DESC`                | `df.orderBy("datum", ascending=False)`        |

## ğŸ—ƒï¸ PrÃ¡ca so Å¡truktÃºrovanÃ½mi formÃ¡tmi

### CSV

```python
df = spark.read.option("header", True).csv("data/objednavky.csv")
```

### JSON

```python
df_json = spark.read.json("data/produkty.json")
```

### Parquet

```python
df_parquet = spark.read.parquet("data/transakcie.parquet")
```


## ğŸ§  OptimalizÃ¡cia Spark SQL

- **Catalyst Optimizer** â€“ analyzuje a optimalizuje logickÃ½ plÃ¡n dopytu.
- **Tungsten Execution Engine** â€“ nÃ­zkoÃºrovÅˆovÃ¡ optimalizÃ¡cia vÃ½poÄtov.
- **Predicate Pushdown** â€“ filtruje dÃ¡ta uÅ¾ pri ich naÄÃ­tavanÃ­.

â¡ï¸ Tieto mechanizmy vÃ½razne zvyÅ¡ujÃº vÃ½kon pri spracovanÃ­ veÄ¾kÃ½ch dÃ¡t.


## ğŸ§ª PokroÄilÃ© SQL: JOIN, funkcie, CASE

```sql
-- Join dvoch tabuliek
SELECT o.id, o.produkt, z.meno
FROM objednavky o
JOIN zakaznici z ON o.zakaznik_id = z.id

-- PrÃ­padovÃ¡ logika
SELECT meno,
       CASE WHEN vek >= 18 THEN 'DospelÃ½' ELSE 'DieÅ¥a' END AS typ
FROM osoby
```

## âœ… Zhrnutie

- Spark SQL umoÅ¾Åˆuje prÃ­stup k dÃ¡tam pomocou znÃ¡mej SQL syntaxe.
- Podporuje integrÃ¡ciu s rÃ´znymi dÃ¡tovÃ½mi formÃ¡tmi (CSV, JSON, Parquet).
- VÃ½kon zabezpeÄuje Catalyst a Tungsten optimalizÃ¡cia.
- SQL dopyty sÃº Äasto kombinovanÃ© s DataFrame API v praxi.

---

<a name="#nastavenie"></a>
# âš™ï¸ 4. Nastavenie prostredia a Spark UI
TÃ¡to kapitola sa venuje praktickÃ©mu nastaveniu Apache Spark v lokÃ¡lnom aj distribuovanom reÅ¾ime. UkÃ¡Å¾eme si tieÅ¾, ako funguje Spark UI â€“ webovÃ© rozhranie pre sledovanie a ladenie vÃ½poÄtov.


## ğŸ’» PoÅ¾iadavky a prÃ­prava prostredia

### âœ… SoftvÃ©rovÃ© poÅ¾iadavky

| Komponent        | OdporÃºÄanÃ¡ verzia        |
|------------------|--------------------------|
| Apache Spark     | 3.5+                     |
| Java (JDK)       | 17 alebo 21              |
| Python           | 3.10+                     |
| PySpark          | najnovÅ¡ia (`pip install`)|
| IDE              | Jetbrains Datalore, Jupyter Notebook, Microsoft Visual Studio Code|

### âœ… InÅ¡talÃ¡cia PySpark

```bash
pip install pyspark
```


## ğŸ—‚ï¸ PremennÃ© prostredia

Pri spÃºÅ¡Å¥anÃ­ Spark aplikÃ¡ciÃ­ je potrebnÃ© nastaviÅ¥ Java prostredie:

```bash
export JAVA_HOME="/path/to/java"
```

Na Windows:

```cmd
set JAVA_HOME=C:\Program Files\Java\jdk-17
```


## ğŸš€ Spustenie SparkSession v Pythone

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder     .appName("MojaSparkAplikacia")     .getOrCreate()
```

### ğŸ§ª Overenie konfigurÃ¡cie

```python
print(spark.version)
print(spark.sparkContext.appName)
```


## ğŸŒ Spark UI â€“ WebovÃ© rozhranie

Po spustenÃ­ aplikÃ¡cie je dostupnÃ© na:

```
http://localhost:4040
```

### ğŸ“Š ÄŒo Spark UI zobrazuje?

| Sekcia        | Popis                                           |
|---------------|--------------------------------------------------|
| Jobs          | PrehÄ¾ad vÅ¡etkÃ½ch spustenÃ½ch Ãºloh                 |
| Stages        | Detaily o jednotlivÃ½ch vÃ½poÄtovÃ½ch fÃ¡zach        |
| Storage       | InformÃ¡cie o RDD a DataFrame v pamÃ¤ti            |
| Environment   | Nastavenia SparkSession a premennÃ©               |
| Executors     | Zoznam executorov a vyuÅ¾itie zdrojov             |
| SQL           | SQL dopyty a ich optimalizovanÃ© plÃ¡ny            |


## ğŸ§ª PrÃ­klad: Spark UI pri spracovanÃ­ CSV

```python
df = spark.read.option("header", "true").csv("data/objednavky.csv")
df.groupBy("produkt").count().show()
```

â¡ï¸ PoÄas vykonania vyÅ¡Å¡ie uvedenÃ©ho dopytu sa automaticky zobrazÃ­ job v Spark UI (4040).


## ğŸ§° UÅ¾itoÄnÃ© nastavenia SparkSession

```python
spark = SparkSession.builder     .appName("Aplikacia")     .config("spark.executor.memory", "2g")     .config("spark.sql.shuffle.partitions", "8")     .getOrCreate()
```

| Parameter                        | Popis                                           |
|----------------------------------|--------------------------------------------------|
| `spark.executor.memory`         | VeÄ¾kosÅ¥ pamÃ¤te pre kaÅ¾dÃ½ executor               |
| `spark.sql.shuffle.partitions`  | PoÄet partÃ­ciÃ­ pri agregÃ¡ciÃ¡ch a joinoch       |
| `spark.driver.memory`           | PamÃ¤Å¥ pre driver proces                         |
| `spark.master`                  | Typ spustenia (napr. `local[*]`, `yarn`, `k8s`) |


## âœ… Zhrnutie

- Spark je moÅ¾nÃ© spustiÅ¥ lokÃ¡lne aj na clustri.
- PySpark beÅ¾Ã­ v Jupyteri alebo ako samostatnÃ½ skript.
- Spark UI poskytuje cennÃ© informÃ¡cie o vÃ½poÄtoch a vÃ½kone.
- Parametre SparkSession ovplyvÅˆujÃº vÃ½kon a pamÃ¤Å¥ovÃ© poÅ¾iadavky.

---
a name="transformacie"></a>

# ğŸ“Š 5. NaÄÃ­tanie dÃ¡t a transformÃ¡cie v Apache Spark

Apache Spark umoÅ¾Åˆuje efektÃ­vne naÄÃ­tanie veÄ¾kÃ©ho mnoÅ¾stva dÃ¡t z rÃ´znych zdrojov a ich spracovanie pomocou transformÃ¡ciÃ­. V tejto kapitole sa zameriame na praktickÃ© prÃ­klady prÃ¡ce so sÃºbormi a najÄastejÅ¡ie transformÃ¡cie nad DataFrame.


## ğŸ“‚ PodporovanÃ© dÃ¡tovÃ© formÃ¡ty

| FormÃ¡t   | Funkcia                                 | PrÃ­klad                                      |
|----------|------------------------------------------|----------------------------------------------|
| CSV      | `spark.read.csv()`                      | `spark.read.option("header", True).csv(...)` |
| JSON     | `spark.read.json()`                     | `spark.read.json("data/produkty.json")`      |
| Parquet  | `spark.read.parquet()`                  | `spark.read.parquet("data/data.parquet")`    |
| ORC      | `spark.read.orc()`                      | `spark.read.orc("data/data.orc")`            |
| JDBC     | `spark.read.jdbc()`                     | NaÄÃ­tanie z relaÄnej databÃ¡zy                |


## ğŸ“¥ PrÃ­klad: NaÄÃ­tanie CSV sÃºboru

```python
df = spark.read.option("header", True).option("inferSchema", True).csv("data/objednavky.csv")
df.printSchema()
df.show(5)
```

## ğŸ”„ TransformÃ¡cie DataFrame

Spark transformÃ¡cie sÃº **lenivÃ©** â€“ nevykonÃ¡vajÃº sa ihneÄ, ale aÅ¾ pri akcii (`show()`, `collect()`, atÄ.).

### âœ… BeÅ¾nÃ© transformÃ¡cie

| OperÃ¡cia         | Popis                                 | Syntax                                      |
|------------------|----------------------------------------|---------------------------------------------|
| `select()`       | VÃ½ber stÄºpcov                         | `df.select("produkt", "cena")`              |
| `filter()`       | Filtrovanie riadkov                   | `df.filter(df["cena"] > 100)`               |
| `withColumn()`   | Pridanie novÃ©ho stÄºpca                | `df.withColumn("DPH", df["cena"] * 0.23)`     |
| `drop()`         | OdstrÃ¡nenie stÄºpca                    | `df.drop("nepotrebny_stlpec")`              |
| `distinct()`     | OdstrÃ¡nenie duplicitnÃ½ch riadkov      | `df.distinct()`                             |
| `groupBy()`      | SkupinovÃ© operÃ¡cie                    | `df.groupBy("kategoria").count()`           |
| `orderBy()`      | Zoradenie                             | `df.orderBy("cena", ascending=False)`       |


## ğŸ§ª PrÃ­klad: Vytvorenie novÃ©ho stÄºpca s DPH

```python
df = df.withColumn("cena_s_DPH", df["cena"] * 1.23)
df.select("produkt", "cena", "cena_s_DPH").show(5)
```


## ğŸ§ª PrÃ­klad: AgregÃ¡cia podÄ¾a kategÃ³rie

```python
df.groupBy("kategoria").agg({"cena": "avg", "id": "count"}).show()
```


## ğŸ§ª PrÃ­klad: Filtrovanie a triedenie

```python
df.filter(df["cena"] > 100).orderBy("cena", ascending=False).show(10)
```


## ğŸ“¦ Ukladanie transformovanÃ½ch dÃ¡t

| FormÃ¡t   | Ukladacia funkcia                        | PrÃ­klad                                      |
|----------|-------------------------------------------|----------------------------------------------|
| CSV      | `df.write.csv()`                         | `df.write.option("header", True).csv(...)`   |
| Parquet  | `df.write.parquet()`                     | `df.write.parquet("output/data")`            |
| JSON     | `df.write.json()`                        | `df.write.json("output/produkty.json")`      |


## âœ… Zhrnutie

- Spark umoÅ¾Åˆuje pracovaÅ¥ s rÃ´znymi typmi dÃ¡tovÃ½ch formÃ¡tov.
- TransformÃ¡cie sÃº deklaratÃ­vne a spÃºÅ¡Å¥ajÃº sa aÅ¾ pri akciÃ¡ch.
- DataFrame API poskytuje bohatÃº sadu funkciÃ­ na spracovanie dÃ¡t.
- DÃ¡ta je moÅ¾nÃ© exportovaÅ¥ spÃ¤Å¥ vo formÃ¡te CSV, JSON, Parquet a ÄalÅ¡Ã­ch.

---

<a name="zdroje"></a>
# ğŸ“š 6. Zdroje a odporÃºÄania pre Apache Spark

V tejto zÃ¡vereÄnej kapitole nÃ¡jdete odporÃºÄanÃ© knihy, dokumentÃ¡ciu, online kurzy a nÃ¡stroje, ktorÃ© vÃ¡m pomÃ´Å¾u rozÅ¡Ã­riÅ¥ znalosti o Apache Spark. TieÅ¾ uvedieme odporÃºÄania pre prax.

## ğŸ“˜ OdporÃºÄanÃ© knihy

| NÃ¡zov | Autor | Popis |
|-------|-------|-------|
| *Learning Spark (2nd Edition)* | Jules S. Damji et al. | VÃ½bornÃ½ Ãºvod do Spark 3 so zameranÃ­m na DataFrame API a Structured Streaming |
| *High Performance Spark* | Holden Karau | OptimalizÃ¡cia vÃ½poÄtov, efektÃ­vne transformÃ¡cie a vÃ½kon |
| *Spark in Action* | Jean-Georges Perrin | PraktickÃ© prÃ­klady a vysvetlenie zÃ¡kladov pre zaÄiatoÄnÃ­kov |
| *Streaming Systems* | Tyler Akidau | TeoretickÃ½ zÃ¡klad pre spracovanie dÃ¡tovÃ½ch tokov v reÃ¡lnom Äase |

## ğŸŒ Online dokumentÃ¡cia a nÃ¡stroje

| Zdroj | Odkaz |
|-------|-------|
| OficiÃ¡lna dokumentÃ¡cia | [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/) |
| API Referencia PySpark | [https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/) |
| Spark GitHub | [https://github.com/apache/spark](https://github.com/apache/spark) |
| Databricks Spark Guide | [https://docs.databricks.com/](https://docs.databricks.com/) |

## ğŸ“ Kurzy a interaktÃ­vne platformy

| Platforma | Kurz / Odkaz |
|-----------|--------------|
| VITA Academy | [https://www.vita.sk/](https://www.vita.sk/) â€“ praktickÃ© kurzy v slovenÄine |
| Datacamp  | Introduction to PySpark |
| Coursera  | Big Data Analysis with Scala and Spark |

## ğŸ› ï¸ VÃ½vojovÃ© prostredia

- **Jupyter Notebook / Lab** â€“ ideÃ¡lne pre rÃ½chle experimentovanie s PySpark
- **VS Code** â€“ podpora PySpark cez rozÅ¡Ã­renia
- **JetBrains DataSpell** â€“ profesionÃ¡lne IDE na prÃ¡cu s dÃ¡tami
- **Databricks Community Edition** â€“ bezplatnÃ¡ platforma pre Spark a ML

## âœ… OdporÃºÄania pre prax

- ğŸ§  Preferujte **DataFrame API** pred RDD pre vÃ½kon a ÄitateÄ¾nosÅ¥
- ğŸ” VyuÅ¾Ã­vajte **Spark UI** (localhost:4040) na ladenie vÃ½konu
- ğŸ›  NauÄte sa optimalizovaÅ¥ dotazy: `cache()`, `repartition()`, `persist()`
- ğŸ—ƒ PouÅ¾Ã­vajte **formÃ¡t Parquet** pre efektÃ­vne ukladanie dÃ¡t
- ğŸ§ª Testujte na malÃ½ch vzorkÃ¡ch a nasadzujte na clustri
- ğŸ“Š Sledujte **plÃ¡n vykonania** (explain) pre optimalizÃ¡ciu dotazov
- ğŸ“¦ Automatizujte pomocou **Airflow, Prefect alebo Luigi**
- ğŸ§± Segmentujte pipeline: ETL, transformÃ¡cie, analytika, ML
- ğŸ”„ Sledujte **verzie Spark a kompatibilitu kniÅ¾nÃ­c**

