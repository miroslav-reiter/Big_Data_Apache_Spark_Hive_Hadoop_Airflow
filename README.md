# ğŸ—„ï¸ Online kurz Big Data (VeÄ¾kÃ© DÃ¡ta), Apache Spark, Hive, Apache Hadoop, Apache Airflow

> PraktickÃ½ kurz pre zaÄiatoÄnÃ­kov â€“ RDD, DataFrame, SparkSQL a distribuovanÃ© spracovanie dÃ¡t

---

## ğŸ“˜ Obsah kurzu
01. [**ğŸ” Ãšvod do veÄ¾kÃ½ch dÃ¡t a Apache Spark**](#uvod-spark)
02. [**ğŸ§± PrÃ¡ca s RDD a DataFrame**](#rdd-dataframe)
03. [**ğŸ§  Spark SQL a dopyty nad dÃ¡tami**](#spark-sql)
04. [**âš™ï¸ Nastavenie prostredia a Spark UI**](#nastavenie)
05. [**ğŸ“Š NaÄÃ­tanie dÃ¡t a transformÃ¡cie**](#transformacie)
06. [**ğŸ“š Zdroje a odporÃºÄania pre Apache Spark**](#zdroje)

---
<a name="uvod-spark"></a>
# ğŸ” 1. Ãšvod do veÄ¾kÃ½ch dÃ¡t a Apache Spark

Apache Spark je vÃ½konnÃ½ open-source engine na spracovanie veÄ¾kÃ½ch dÃ¡t v reÃ¡lnom Äase. Podporuje paralelnÃ© vÃ½poÄty v pamÃ¤ti a je Å¡iroko pouÅ¾Ã­vanÃ½ v oblasti dÃ¡tovej analytiky, strojovÃ©ho uÄenia a streamovania.

---

## ğŸ“Š ÄŒo sÃº veÄ¾kÃ© dÃ¡ta â€“ model 5V

VeÄ¾kÃ© dÃ¡ta sÃº charakterizovanÃ© nasledujÃºcimi 5 vlastnosÅ¥ami:

| ğŸ†” **VlastnosÅ¥** | ğŸ“Œ **Popis**                                 | ğŸ’¡ **PrÃ­klad**                                |
|------------------|----------------------------------------------|-----------------------------------------------|
| ğŸ“¦ Volume         | Objem dÃ¡t â€“ terabajty aÅ¾ petabajty           | TransakÄnÃ© dÃ¡ta, zÃ¡znamy zo senzorov          |
| âš¡ Velocity       | RÃ½chlosÅ¥ generovania dÃ¡t                     | DÃ¡tovÃ© toky z IoT zariadenÃ­, streamy videa    |
| ğŸ§© Variety        | RÃ´znorodosÅ¥ dÃ¡t â€“ Å¡truktÃºrovanÃ© aj neÅ¡trukt. | CSV, JSON, obrÃ¡zky, logy, XML                 |
| âœ… Veracity       | VierohodnosÅ¥ a kvalita dÃ¡t                   | ChÃ½bajÃºce hodnoty, nekonzistentnÃ© zÃ¡znamy     |
| ğŸ’° Value          | Hodnota, ktorÃº je moÅ¾nÃ© z dÃ¡t zÃ­skaÅ¥         | AnalÃ½zy zÃ¡kaznÃ­kov, predikcie, odporÃºÄania    |

---

## âš™ï¸ PreÄo Apache Spark?

| ğŸ’¡ VlastnosÅ¥         | ğŸ”¥ Apache Spark                                      |
|----------------------|-----------------------------------------------------|
| ğŸ’¾ Spracovanie       | V pamÃ¤ti (in-memory), rÃ½chlejÅ¡ie ako Hadoop         |
| ğŸ§  ProgramovacÃ­ model| RDD, DataFrame, SQL, MLlib, GraphX                  |
| ğŸŒ Podpora jazykov   | Python, Scala, Java, R                              |
| ğŸ“ˆ VyuÅ¾itie          | Batch, stream, interaktÃ­vne, strojovÃ© uÄenie        |
| ğŸ“š EkosystÃ©m         | BohatÃ¡ dokumentÃ¡cia, rozÅ¡Ã­renia, kompatibilita     |

---

## ğŸ—ï¸ ArchitektÃºra Apache Spark

- **Driver Program** â€“ riadi vykonÃ¡vanie a vytvÃ¡ra DAG
- **Cluster Manager** â€“ prideÄ¾uje zdroje (napr. YARN, Kubernetes)
- **Executors** â€“ vykonÃ¡vajÃº Ãºlohy a spracÃºvajÃº dÃ¡ta
- **Tasks** â€“ jednotky paralelnÃ©ho vÃ½poÄtu

ğŸŒ€ **DAG (Directed Acyclic Graph)** â€“ reprezentuje logiku vÃ½poÄtu ako necyklickÃ½ graf zÃ¡vislostÃ­.

---

## ğŸ§± Moduly Apache Spark

| Modul            | Popis                                             |
|------------------|---------------------------------------------------|
| `Spark Core`     | ZÃ¡kladnÃ© API, sprÃ¡va pamÃ¤te a plÃ¡novanie vÃ½poÄtu |
| `Spark SQL`      | Dopytovanie cez SQL a DataFrame API              |
| `Spark MLlib`    | NÃ¡stroje pre strojovÃ© uÄenie                     |
| `Spark Streaming`| StreamovÃ© (real-time) spracovanie                |
| `GraphX`         | GrafovÃ© vÃ½poÄty a analÃ½zy                        |

---

## ğŸ“¦ PodporovanÃ© formÃ¡ty a zdroje dÃ¡t

- **FormÃ¡ty**: CSV, JSON, Parquet, Avro, ORC
- **Zdroje**: HDFS, S3, JDBC, Kafka, lokÃ¡lne sÃºbory, NoSQL databÃ¡zy

---

## ğŸ§  PrÃ­klady vyuÅ¾itia

| ğŸ¢ Odvetvie        | ğŸ“ˆ PrÃ­pad pouÅ¾itia                              |
|--------------------|--------------------------------------------------|
| FinTech             | Detekcia podvodov v reÃ¡lnom Äase                |
| E-commerce          | OdporÃºÄacie systÃ©my, personalizÃ¡cia ponÃºk       |
| ZdravotnÃ­ctvo       | Predikcia diagnÃ³z na zÃ¡klade historickÃ½ch dÃ¡t   |
| VÃ½roba / IoT        | PrediktÃ­vna ÃºdrÅ¾ba, sledovanie vÃ½konu strojov   |
| Marketing           | SegmentÃ¡cia zÃ¡kaznÃ­kov, analÃ½za sprÃ¡vania       |

---

## âœ… Zhrnutie

- Apache Spark je ideÃ¡lny nÃ¡stroj pre prÃ¡cu s veÄ¾kÃ½mi dÃ¡tami v rÃ´znych formÃ¡toch.
- PonÃºka vysokÃ½ vÃ½kon, Å¡kÃ¡lovateÄ¾nosÅ¥ a bohatÃ½ ekosystÃ©m nÃ¡strojov.
- Je vhodnÃ½ pre dÃ¡vkovÃ© aj real-time aplikÃ¡cie v mnohÃ½ch oblastiach.

---

<a name="rdd-dataframe"></a>
# ğŸ§± 2. PrÃ¡ca s RDD a DataFrame v Apache Spark

Apache Spark umoÅ¾Åˆuje dve hlavnÃ© abstrakcie pre prÃ¡cu s dÃ¡tami: **RDD (Resilient Distributed Dataset)** a **DataFrame**. V tejto kapitole si vysvetlÃ­me rozdiely, vÃ½hody a praktickÃ© prÃ­klady pouÅ¾itia oboch.

---

## ğŸ§  ÄŒo je RDD?

**RDD (Resilient Distributed Dataset)** je nÃ­zkoÃºrovÅˆovÃ¡, nemennÃ¡ kolekcia objektov, ktorÃ¡ sa distribuuje medzi uzly v klastri.

### âš™ï¸ Vlastnosti RDD:

- Immutability â€“ nemennÃ© Å¡truktÃºry
- ParalelnÃ© spracovanie
- Fault-tolerance â€“ automatickÃ¡ replikÃ¡cia a zotavenie
- Podpora funkcionÃ¡lnych operÃ¡ciÃ­ ako `map()`, `filter()`, `reduce()`

### ğŸ§ª PrÃ­klad: ZÃ¡kladnÃ¡ prÃ¡ca s RDD

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_squared = rdd.map(lambda x: x * x)
print(rdd_squared.collect())  # VÃ½stup: [1, 4, 9, 16, 25]
```

---

## ğŸ“„ TransformÃ¡cie a akcie na RDD

| Typ operÃ¡cie   | PrÃ­klad           | Popis                                       |
|----------------|-------------------|---------------------------------------------|
| TransformÃ¡cia  | `map()`, `filter()`| VytvÃ¡ra novÃ½ RDD                            |
| Akcia          | `collect()`, `count()` | SpustÃ­ vÃ½poÄet a vrÃ¡ti vÃ½sledok do drivera  |

---

## ğŸ“˜ ÄŒo je DataFrame?

**DataFrame** je vyÅ¡Å¡ia abstrakcia nad RDD s metadÃ¡tami (schema), podobnÃ¡ Pandas alebo SQL tabuÄ¾ke.

### ğŸ§¾ VÃ½hody DataFrame:

- OptimalizÃ¡cia pomocou Catalyst engine
- VÃ½razne rÃ½chlejÅ¡ie spracovanie ako RDD
- MoÅ¾nosÅ¥ pouÅ¾Ã­vaÅ¥ SQL-like syntax
- AutomatickÃ© spracovanie schÃ©my

### ğŸ§ª PrÃ­klad: Vytvorenie DataFrame

```python
from pyspark.sql import Row

df = spark.createDataFrame([Row(meno="Anna", vek=25), Row(meno="JÃ¡n", vek=32)])
df.show()
```

---

## ğŸ” BeÅ¾nÃ© operÃ¡cie s DataFrame

```python
df.filter(df.vek > 30).select("meno").show()
df.groupBy("vek").count().show()
```

| OperÃ¡cia          | Syntax                                     | Popis                           |
|-------------------|--------------------------------------------|----------------------------------|
| Filtrovanie       | `df.filter(df.vek > 30)`                   | VÃ½ber podÄ¾a podmienky           |
| VÃ½ber stÄºpcov     | `df.select("meno", "vek")`                 | VÃ½ber konkrÃ©tnych stÄºpcov       |
| AgregÃ¡cia         | `df.groupBy("vek").count()`                | SkupinovÃ© vÃ½poÄty               |
| Triedenie         | `df.orderBy("vek", ascending=False)`       | Zoradenie podÄ¾a hodnoty         |

---

## ğŸ” Porovnanie RDD vs. DataFrame

| VlastnosÅ¥             | RDD                                 | DataFrame                         |
|------------------------|--------------------------------------|------------------------------------|
| API Å¡tÃ½l              | FunkcionÃ¡lny (map, reduce)           | DeklaratÃ­vny (SQL-like)            |
| OptimalizÃ¡cia         | Bez optimalizÃ¡cie                    | Catalyst + Tungsten optimizÃ¡cia    |
| VÃ½kon                 | PomalÅ¡Ã­                              | RÃ½chlejÅ¡Ã­                          |
| ÄŒitateÄ¾nosÅ¥           | NiÅ¾Å¡ia (viac kÃ³du)                   | VyÅ¡Å¡ia (kompaktnejÅ¡Ã­ kÃ³d)          |
| PrÃ­stup k schÃ©me      | Nie                                  | Ãno                                |

---

## ğŸ”ƒ Prechod z RDD na DataFrame a spÃ¤Å¥

```python
# RDD â†’ DataFrame
from pyspark.sql import Row
rdd = spark.sparkContext.parallelize([Row(meno="Eva", vek=29)])
df = spark.createDataFrame(rdd)

# DataFrame â†’ RDD
rdd2 = df.rdd
```

---

## ğŸ§ª UkÃ¡Å¾ka prÃ¡ce s CSV sÃºborom ako DataFrame

```python
df_csv = spark.read.csv("data/osoby.csv", header=True, inferSchema=True)
df_csv.printSchema()
df_csv.select("meno", "vek").show()
```

---

## âœ… Zhrnutie

- **RDD** poskytuje nÃ­zkoÃºrovÅˆovÃº kontrolu nad dÃ¡tami, vhodnÃ© na zloÅ¾itÃ© transformÃ¡cie.
- **DataFrame** poskytuje vyÅ¡Å¡Ã­ vÃ½kon, ÄitateÄ¾nosÅ¥ a podporu SQL.
- V modernÃ½ch aplikÃ¡ciÃ¡ch sa odporÃºÄa pouÅ¾Ã­vaÅ¥ **DataFrame API**, ak nie je potrebnÃ© nieÄo Å¡pecifickÃ© z RDD.

---

<a name="spark-sql"></a>

# ğŸ§  3. Spark SQL a dopyty nad dÃ¡tami

Spark SQL je modul Apache Spark, ktorÃ½ umoÅ¾Åˆuje spracovanie Å¡truktÃºrovanÃ½ch dÃ¡t pomocou SQL syntaxe alebo DataFrame API. Kombinuje vÃ½konnosÅ¥ Spark enginu s jednoduchosÅ¥ou SQL.

---

## ğŸ“‹ ÄŒo je Spark SQL?

Spark SQL umoÅ¾Åˆuje:

- vykonÃ¡vaÅ¥ SQL dopyty priamo nad veÄ¾kÃ½mi dÃ¡tami,
- manipulovaÅ¥ so Å¡truktÃºrovanÃ½mi dÃ¡tami pomocou DataFrame API,
- pracovaÅ¥ s rÃ´znymi zdrojmi dÃ¡t ako CSV, Parquet, Hive, JDBC.

---

## ğŸ”§ Vytvorenie DataFrame tabuÄ¾ky

### ğŸ§ª PrÃ­klad: NaÄÃ­tanie CSV a registrÃ¡cia ako tabuÄ¾ka

```python
df = spark.read.option("header", "true").csv("data/objednavky.csv")
df.createOrReplaceTempView("objednavky")
```

Po registrÃ¡cii mÃ´Å¾ete nad `objednavky` spÃºÅ¡Å¥aÅ¥ SQL dopyty.

---

## ğŸ§ª PrÃ­klady SQL dopytov

```python
# VÃ½ber vÅ¡etkÃ½ch stÄºpcov
spark.sql("SELECT * FROM objednavky").show()

# Filtrovanie podÄ¾a hodnoty
spark.sql("SELECT * FROM objednavky WHERE cena > 100").show()

# SkupinovÃ© vÃ½poÄty
spark.sql("SELECT produkt, COUNT(*) AS pocet FROM objednavky GROUP BY produkt").show()
```

---

## ğŸ“Š Porovnanie: SQL vs. DataFrame API

| OperÃ¡cia                     | SQL syntax                                                    | DataFrame API                                 |
|------------------------------|----------------------------------------------------------------|-----------------------------------------------|
| VÃ½ber                        | `SELECT meno FROM zakaznici`                                  | `df.select("meno")`                           |
| Filtrovanie                  | `SELECT * FROM objednavky WHERE cena > 100`                   | `df.filter(df.cena > 100)`                    |
| AgregÃ¡cia                    | `SELECT AVG(cena) FROM objednavky`                            | `df.agg({"cena": "avg"})`                     |
| Zoskupenie                   | `SELECT produkt, COUNT(*) FROM objednavky GROUP BY produkt`   | `df.groupBy("produkt").count()`              |
| Triedenie                    | `SELECT * FROM objednavky ORDER BY datum DESC`                | `df.orderBy("datum", ascending=False)`        |

---

## ğŸ—ƒï¸ PrÃ¡ca so Å¡truktÃºrovanÃ½mi formÃ¡tmi

### CSV

```python
df = spark.read.option("header", True).csv("data/objednavky.csv")
```

### JSON

```python
df_json = spark.read.json("data/produkty.json")
```

### Parquet

```python
df_parquet = spark.read.parquet("data/transakcie.parquet")
```

---

## ğŸ§  OptimalizÃ¡cia Spark SQL

- **Catalyst Optimizer** â€“ analyzuje a optimalizuje logickÃ½ plÃ¡n dopytu.
- **Tungsten Execution Engine** â€“ nÃ­zkoÃºrovÅˆovÃ¡ optimalizÃ¡cia vÃ½poÄtov.
- **Predicate Pushdown** â€“ filtruje dÃ¡ta uÅ¾ pri ich naÄÃ­tavanÃ­.

â¡ï¸ Tieto mechanizmy vÃ½razne zvyÅ¡ujÃº vÃ½kon pri spracovanÃ­ veÄ¾kÃ½ch dÃ¡t.

---

## ğŸ§ª PokroÄilÃ© SQL: JOIN, funkcie, CASE

```sql
-- Join dvoch tabuliek
SELECT o.id, o.produkt, z.meno
FROM objednavky o
JOIN zakaznici z ON o.zakaznik_id = z.id

-- PrÃ­padovÃ¡ logika
SELECT meno,
       CASE WHEN vek >= 18 THEN 'DospelÃ½' ELSE 'DieÅ¥a' END AS typ
FROM osoby
```

---

## âœ… Zhrnutie

- Spark SQL umoÅ¾Åˆuje prÃ­stup k dÃ¡tam pomocou znÃ¡mej SQL syntaxe.
- Podporuje integrÃ¡ciu s rÃ´znymi dÃ¡tovÃ½mi formÃ¡tmi (CSV, JSON, Parquet).
- VÃ½kon zabezpeÄuje Catalyst a Tungsten optimalizÃ¡cia.
- SQL dopyty sÃº Äasto kombinovanÃ© s DataFrame API v praxi.

---

<a name="#nastavenie"></a>
# âš™ï¸ 4. Nastavenie prostredia a Spark UI

TÃ¡to kapitola sa venuje praktickÃ©mu nastaveniu Apache Spark v lokÃ¡lnom aj distribuovanom reÅ¾ime. UkÃ¡Å¾eme si tieÅ¾, ako funguje Spark UI â€“ webovÃ© rozhranie pre sledovanie a ladenie vÃ½poÄtov.

---

## ğŸ’» PoÅ¾iadavky a prÃ­prava prostredia

### âœ… SoftvÃ©rovÃ© poÅ¾iadavky

| Komponent        | OdporÃºÄanÃ¡ verzia        |
|------------------|--------------------------|
| Apache Spark     | 3.5+                     |
| Java (JDK)       | 17 alebo 21              |
| Python           | 3.8+                     |
| PySpark          | najnovÅ¡ia (`pip install`)|
| IDE              | VS Code, Jupyter Notebook|

### âœ… InÅ¡talÃ¡cia PySpark

```bash
pip install pyspark
```

---

## ğŸ—‚ï¸ PremennÃ© prostredia

Pri spÃºÅ¡Å¥anÃ­ Spark aplikÃ¡ciÃ­ je potrebnÃ© nastaviÅ¥ Java prostredie:

```bash
export JAVA_HOME="/path/to/java"
```

Na Windows:

```cmd
set JAVA_HOME=C:\Program Files\Java\jdk-17
```

---

## ğŸš€ Spustenie SparkSession v Pythone

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder     .appName("MojaSparkAplikacia")     .getOrCreate()
```

### ğŸ§ª Overenie konfigurÃ¡cie

```python
print(spark.version)
print(spark.sparkContext.appName)
```

---

## ğŸŒ Spark UI â€“ WebovÃ© rozhranie

Po spustenÃ­ aplikÃ¡cie je dostupnÃ© na:

```
http://localhost:4040
```

### ğŸ“Š ÄŒo Spark UI zobrazuje?

| Sekcia        | Popis                                           |
|---------------|--------------------------------------------------|
| Jobs          | PrehÄ¾ad vÅ¡etkÃ½ch spustenÃ½ch Ãºloh                 |
| Stages        | Detaily o jednotlivÃ½ch vÃ½poÄtovÃ½ch fÃ¡zach        |
| Storage       | InformÃ¡cie o RDD a DataFrame v pamÃ¤ti            |
| Environment   | Nastavenia SparkSession a premennÃ©               |
| Executors     | Zoznam executorov a vyuÅ¾itie zdrojov             |
| SQL           | SQL dopyty a ich optimalizovanÃ© plÃ¡ny            |

---

## ğŸ§ª PrÃ­klad: Spark UI pri spracovanÃ­ CSV

```python
df = spark.read.option("header", "true").csv("data/objednavky.csv")
df.groupBy("produkt").count().show()
```

â¡ï¸ PoÄas vykonania vyÅ¡Å¡ie uvedenÃ©ho dopytu sa automaticky zobrazÃ­ job v Spark UI (4040).

---

## ğŸ§° UÅ¾itoÄnÃ© nastavenia SparkSession

```python
spark = SparkSession.builder     .appName("Aplikacia")     .config("spark.executor.memory", "2g")     .config("spark.sql.shuffle.partitions", "8")     .getOrCreate()
```

| Parameter                        | Popis                                           |
|----------------------------------|--------------------------------------------------|
| `spark.executor.memory`         | VeÄ¾kosÅ¥ pamÃ¤te pre kaÅ¾dÃ½ executor               |
| `spark.sql.shuffle.partitions`  | PoÄet partÃ­ciÃ­ pri agregÃ¡ciÃ¡ch a joinoch       |
| `spark.driver.memory`           | PamÃ¤Å¥ pre driver proces                         |
| `spark.master`                  | Typ spustenia (napr. `local[*]`, `yarn`, `k8s`) |

---

## âœ… Zhrnutie

- Spark je moÅ¾nÃ© spustiÅ¥ lokÃ¡lne aj na clustri.
- PySpark beÅ¾Ã­ v Jupyteri alebo ako samostatnÃ½ skript.
- Spark UI poskytuje cennÃ© informÃ¡cie o vÃ½poÄtoch a vÃ½kone.
- Parametre SparkSession ovplyvÅˆujÃº vÃ½kon a pamÃ¤Å¥ovÃ© poÅ¾iadavky.

---
a name="transformacie"></a>

# ğŸ“Š 5. NaÄÃ­tanie dÃ¡t a transformÃ¡cie v Apache Spark

Apache Spark umoÅ¾Åˆuje efektÃ­vne naÄÃ­tanie veÄ¾kÃ©ho mnoÅ¾stva dÃ¡t z rÃ´znych zdrojov a ich spracovanie pomocou transformÃ¡ciÃ­. V tejto kapitole sa zameriame na praktickÃ© prÃ­klady prÃ¡ce so sÃºbormi a najÄastejÅ¡ie transformÃ¡cie nad DataFrame.

---

## ğŸ“‚ PodporovanÃ© dÃ¡tovÃ© formÃ¡ty

| FormÃ¡t   | Funkcia                                 | PrÃ­klad                                      |
|----------|------------------------------------------|----------------------------------------------|
| CSV      | `spark.read.csv()`                      | `spark.read.option("header", True).csv(...)` |
| JSON     | `spark.read.json()`                     | `spark.read.json("data/produkty.json")`      |
| Parquet  | `spark.read.parquet()`                  | `spark.read.parquet("data/data.parquet")`    |
| ORC      | `spark.read.orc()`                      | `spark.read.orc("data/data.orc")`            |
| JDBC     | `spark.read.jdbc()`                     | NaÄÃ­tanie z relaÄnej databÃ¡zy                |

---

## ğŸ“¥ PrÃ­klad: NaÄÃ­tanie CSV sÃºboru

```python
df = spark.read.option("header", True).option("inferSchema", True).csv("data/objednavky.csv")
df.printSchema()
df.show(5)
```

---

## ğŸ”„ TransformÃ¡cie DataFrame

Spark transformÃ¡cie sÃº **lenivÃ©** â€“ nevykonÃ¡vajÃº sa ihneÄ, ale aÅ¾ pri akcii (`show()`, `collect()`, atÄ.).

### âœ… BeÅ¾nÃ© transformÃ¡cie

| OperÃ¡cia         | Popis                                 | Syntax                                      |
|------------------|----------------------------------------|---------------------------------------------|
| `select()`       | VÃ½ber stÄºpcov                         | `df.select("produkt", "cena")`              |
| `filter()`       | Filtrovanie riadkov                   | `df.filter(df["cena"] > 100)`               |
| `withColumn()`   | Pridanie novÃ©ho stÄºpca                | `df.withColumn("DPH", df["cena"] * 0.2)`     |
| `drop()`         | OdstrÃ¡nenie stÄºpca                    | `df.drop("nepotrebny_stlpec")`              |
| `distinct()`     | OdstrÃ¡nenie duplicitnÃ½ch riadkov      | `df.distinct()`                             |
| `groupBy()`      | SkupinovÃ© operÃ¡cie                    | `df.groupBy("kategoria").count()`           |
| `orderBy()`      | Zoradenie                             | `df.orderBy("cena", ascending=False)`       |

---

## ğŸ§ª PrÃ­klad: Vytvorenie novÃ©ho stÄºpca s DPH

```python
df = df.withColumn("cena_s_DPH", df["cena"] * 1.2)
df.select("produkt", "cena", "cena_s_DPH").show(5)
```

---

## ğŸ§ª PrÃ­klad: AgregÃ¡cia podÄ¾a kategÃ³rie

```python
df.groupBy("kategoria").agg({"cena": "avg", "id": "count"}).show()
```

---

## ğŸ§ª PrÃ­klad: Filtrovanie a triedenie

```python
df.filter(df["cena"] > 100).orderBy("cena", ascending=False).show(10)
```

---

## ğŸ“¦ Ukladanie transformovanÃ½ch dÃ¡t

| FormÃ¡t   | Ukladacia funkcia                        | PrÃ­klad                                      |
|----------|-------------------------------------------|----------------------------------------------|
| CSV      | `df.write.csv()`                         | `df.write.option("header", True).csv(...)`   |
| Parquet  | `df.write.parquet()`                     | `df.write.parquet("output/data")`            |
| JSON     | `df.write.json()`                        | `df.write.json("output/produkty.json")`      |

---

## âœ… Zhrnutie

- Spark umoÅ¾Åˆuje pracovaÅ¥ s rÃ´znymi typmi dÃ¡tovÃ½ch formÃ¡tov.
- TransformÃ¡cie sÃº deklaratÃ­vne a spÃºÅ¡Å¥ajÃº sa aÅ¾ pri akciÃ¡ch.
- DataFrame API poskytuje bohatÃº sadu funkciÃ­ na spracovanie dÃ¡t.
- DÃ¡ta je moÅ¾nÃ© exportovaÅ¥ spÃ¤Å¥ vo formÃ¡te CSV, JSON, Parquet a ÄalÅ¡Ã­ch.

---

