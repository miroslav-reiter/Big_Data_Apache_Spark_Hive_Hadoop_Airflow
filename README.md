# ğŸ—„ï¸ Online kurz Big Data (VeÄ¾kÃ© DÃ¡ta), Apache Spark, Hive, Apache Hadoop, Apache Airflow

> PraktickÃ½ kurz pre zaÄiatoÄnÃ­kov â€“ RDD, DataFrame, SparkSQL a distribuovanÃ© spracovanie dÃ¡t

---

## ğŸ“˜ Obsah kurzu

01. [**ğŸ” Ãšvod do veÄ¾kÃ½ch dÃ¡t a Apache Spark**](#uvod-spark)
02. [**ğŸ§± PrÃ¡ca s RDD a DataFrame**](#rdd-dataframe)
03. [**ğŸ§  Spark SQL a dopyty nad dÃ¡tami**](#spark-sql)
04. [**âš™ï¸ Nastavenie prostredia a Spark UI**](#nastavenie)
05. [**ğŸ“Š NaÄÃ­tanie dÃ¡t a transformÃ¡cie**](#transformacie)
06. [**ğŸ“š Zdroje a odporÃºÄania pre Apache Spark**](#zdroje)

---

<a name="uvod-spark"></a>

# ğŸ” 1. Ãšvod do veÄ¾kÃ½ch dÃ¡t a Apache Spark

Apache Spark je vÃ½konnÃ½ open-source engine na spracovanie veÄ¾kÃ½ch dÃ¡t v reÃ¡lnom Äase. Podporuje paralelnÃ© vÃ½poÄty v pamÃ¤ti a je Å¡iroko pouÅ¾Ã­vanÃ½ v oblasti dÃ¡tovej analytiky, strojovÃ©ho uÄenia a streamovania.

---

## ğŸ“Š ÄŒo sÃº veÄ¾kÃ© dÃ¡ta â€“ model 5V

VeÄ¾kÃ© dÃ¡ta sÃº charakterizovanÃ© nasledujÃºcimi 5 vlastnosÅ¥ami:

| ğŸ†” **VlastnosÅ¥** | ğŸ“Œ **Popis**                                 | ğŸ’¡ **PrÃ­klad**                                |
|------------------|----------------------------------------------|-----------------------------------------------|
| ğŸ“¦ Volume         | Objem dÃ¡t â€“ terabajty aÅ¾ petabajty           | TransakÄnÃ© dÃ¡ta, zÃ¡znamy zo senzorov          |
| âš¡ Velocity       | RÃ½chlosÅ¥ generovania dÃ¡t                     | DÃ¡tovÃ© toky z IoT zariadenÃ­, streamy videa    |
| ğŸ§© Variety        | RÃ´znorodosÅ¥ dÃ¡t â€“ Å¡truktÃºrovanÃ© aj neÅ¡trukt. | CSV, JSON, obrÃ¡zky, logy, XML                 |
| âœ… Veracity       | VierohodnosÅ¥ a kvalita dÃ¡t                   | ChÃ½bajÃºce hodnoty, nekonzistentnÃ© zÃ¡znamy     |
| ğŸ’° Value          | Hodnota, ktorÃº je moÅ¾nÃ© z dÃ¡t zÃ­skaÅ¥         | AnalÃ½zy zÃ¡kaznÃ­kov, predikcie, odporÃºÄania    |

---

## âš™ï¸ PreÄo Apache Spark?

| ğŸ’¡ VlastnosÅ¥         | ğŸ”¥ Apache Spark                                      |
|----------------------|-----------------------------------------------------|
| ğŸ’¾ Spracovanie       | V pamÃ¤ti (in-memory), rÃ½chlejÅ¡ie ako Hadoop         |
| ğŸ§  ProgramovacÃ­ model| RDD, DataFrame, SQL, MLlib, GraphX                  |
| ğŸŒ Podpora jazykov   | Python, Scala, Java, R                              |
| ğŸ“ˆ VyuÅ¾itie          | Batch, stream, interaktÃ­vne, strojovÃ© uÄenie        |
| ğŸ“š EkosystÃ©m         | BohatÃ¡ dokumentÃ¡cia, rozÅ¡Ã­renia, kompatibilita     |

---

## ğŸ—ï¸ ArchitektÃºra Apache Spark

- **Driver Program** â€“ riadi vykonÃ¡vanie a vytvÃ¡ra DAG
- **Cluster Manager** â€“ prideÄ¾uje zdroje (napr. YARN, Kubernetes)
- **Executors** â€“ vykonÃ¡vajÃº Ãºlohy a spracÃºvajÃº dÃ¡ta
- **Tasks** â€“ jednotky paralelnÃ©ho vÃ½poÄtu

ğŸŒ€ **DAG (Directed Acyclic Graph)** â€“ reprezentuje logiku vÃ½poÄtu ako necyklickÃ½ graf zÃ¡vislostÃ­.

---

## ğŸ§± Moduly Apache Spark

| Modul            | Popis                                             |
|------------------|---------------------------------------------------|
| `Spark Core`     | ZÃ¡kladnÃ© API, sprÃ¡va pamÃ¤te a plÃ¡novanie vÃ½poÄtu |
| `Spark SQL`      | Dopytovanie cez SQL a DataFrame API              |
| `Spark MLlib`    | NÃ¡stroje pre strojovÃ© uÄenie                     |
| `Spark Streaming`| StreamovÃ© (real-time) spracovanie                |
| `GraphX`         | GrafovÃ© vÃ½poÄty a analÃ½zy                        |

---

## ğŸ“¦ PodporovanÃ© formÃ¡ty a zdroje dÃ¡t

- **FormÃ¡ty**: CSV, JSON, Parquet, Avro, ORC
- **Zdroje**: HDFS, S3, JDBC, Kafka, lokÃ¡lne sÃºbory, NoSQL databÃ¡zy

---

## ğŸ§  PrÃ­klady vyuÅ¾itia

| ğŸ¢ Odvetvie        | ğŸ“ˆ PrÃ­pad pouÅ¾itia                              |
|--------------------|--------------------------------------------------|
| FinTech             | Detekcia podvodov v reÃ¡lnom Äase                |
| E-commerce          | OdporÃºÄacie systÃ©my, personalizÃ¡cia ponÃºk       |
| ZdravotnÃ­ctvo       | Predikcia diagnÃ³z na zÃ¡klade historickÃ½ch dÃ¡t   |
| VÃ½roba / IoT        | PrediktÃ­vna ÃºdrÅ¾ba, sledovanie vÃ½konu strojov   |
| Marketing           | SegmentÃ¡cia zÃ¡kaznÃ­kov, analÃ½za sprÃ¡vania       |

---

## âœ… Zhrnutie

- Apache Spark je ideÃ¡lny nÃ¡stroj pre prÃ¡cu s veÄ¾kÃ½mi dÃ¡tami v rÃ´znych formÃ¡toch.
- PonÃºka vysokÃ½ vÃ½kon, Å¡kÃ¡lovateÄ¾nosÅ¥ a bohatÃ½ ekosystÃ©m nÃ¡strojov.
- Je vhodnÃ½ pre dÃ¡vkovÃ© aj real-time aplikÃ¡cie v mnohÃ½ch oblastiach.

---

<a name="rdd-dataframe"></a>
# ğŸ§± 2. PrÃ¡ca s RDD a DataFrame v Apache Spark

Apache Spark umoÅ¾Åˆuje dve hlavnÃ© abstrakcie pre prÃ¡cu s dÃ¡tami: **RDD (Resilient Distributed Dataset)** a **DataFrame**. V tejto kapitole si vysvetlÃ­me rozdiely, vÃ½hody a praktickÃ© prÃ­klady pouÅ¾itia oboch.

---

## ğŸ§  ÄŒo je RDD?

**RDD (Resilient Distributed Dataset)** je nÃ­zkoÃºrovÅˆovÃ¡, nemennÃ¡ kolekcia objektov, ktorÃ¡ sa distribuuje medzi uzly v klastri.

### âš™ï¸ Vlastnosti RDD:

- Immutability â€“ nemennÃ© Å¡truktÃºry
- ParalelnÃ© spracovanie
- Fault-tolerance â€“ automatickÃ¡ replikÃ¡cia a zotavenie
- Podpora funkcionÃ¡lnych operÃ¡ciÃ­ ako `map()`, `filter()`, `reduce()`

### ğŸ§ª PrÃ­klad: ZÃ¡kladnÃ¡ prÃ¡ca s RDD

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_squared = rdd.map(lambda x: x * x)
print(rdd_squared.collect())  # VÃ½stup: [1, 4, 9, 16, 25]
```

---

## ğŸ“„ TransformÃ¡cie a akcie na RDD

| Typ operÃ¡cie   | PrÃ­klad           | Popis                                       |
|----------------|-------------------|---------------------------------------------|
| TransformÃ¡cia  | `map()`, `filter()`| VytvÃ¡ra novÃ½ RDD                            |
| Akcia          | `collect()`, `count()` | SpustÃ­ vÃ½poÄet a vrÃ¡ti vÃ½sledok do drivera  |

---

## ğŸ“˜ ÄŒo je DataFrame?

**DataFrame** je vyÅ¡Å¡ia abstrakcia nad RDD s metadÃ¡tami (schema), podobnÃ¡ Pandas alebo SQL tabuÄ¾ke.

### ğŸ§¾ VÃ½hody DataFrame:

- OptimalizÃ¡cia pomocou Catalyst engine
- VÃ½razne rÃ½chlejÅ¡ie spracovanie ako RDD
- MoÅ¾nosÅ¥ pouÅ¾Ã­vaÅ¥ SQL-like syntax
- AutomatickÃ© spracovanie schÃ©my

### ğŸ§ª PrÃ­klad: Vytvorenie DataFrame

```python
from pyspark.sql import Row

df = spark.createDataFrame([Row(meno="Anna", vek=25), Row(meno="JÃ¡n", vek=32)])
df.show()
```

---

## ğŸ” BeÅ¾nÃ© operÃ¡cie s DataFrame

```python
df.filter(df.vek > 30).select("meno").show()
df.groupBy("vek").count().show()
```

| OperÃ¡cia          | Syntax                                     | Popis                           |
|-------------------|--------------------------------------------|----------------------------------|
| Filtrovanie       | `df.filter(df.vek > 30)`                   | VÃ½ber podÄ¾a podmienky           |
| VÃ½ber stÄºpcov     | `df.select("meno", "vek")`                 | VÃ½ber konkrÃ©tnych stÄºpcov       |
| AgregÃ¡cia         | `df.groupBy("vek").count()`                | SkupinovÃ© vÃ½poÄty               |
| Triedenie         | `df.orderBy("vek", ascending=False)`       | Zoradenie podÄ¾a hodnoty         |

---

## ğŸ” Porovnanie RDD vs. DataFrame

| VlastnosÅ¥             | RDD                                 | DataFrame                         |
|------------------------|--------------------------------------|------------------------------------|
| API Å¡tÃ½l              | FunkcionÃ¡lny (map, reduce)           | DeklaratÃ­vny (SQL-like)            |
| OptimalizÃ¡cia         | Bez optimalizÃ¡cie                    | Catalyst + Tungsten optimizÃ¡cia    |
| VÃ½kon                 | PomalÅ¡Ã­                              | RÃ½chlejÅ¡Ã­                          |
| ÄŒitateÄ¾nosÅ¥           | NiÅ¾Å¡ia (viac kÃ³du)                   | VyÅ¡Å¡ia (kompaktnejÅ¡Ã­ kÃ³d)          |
| PrÃ­stup k schÃ©me      | Nie                                  | Ãno                                |

---

## ğŸ”ƒ Prechod z RDD na DataFrame a spÃ¤Å¥

```python
# RDD â†’ DataFrame
from pyspark.sql import Row
rdd = spark.sparkContext.parallelize([Row(meno="Eva", vek=29)])
df = spark.createDataFrame(rdd)

# DataFrame â†’ RDD
rdd2 = df.rdd
```

---

## ğŸ§ª UkÃ¡Å¾ka prÃ¡ce s CSV sÃºborom ako DataFrame

```python
df_csv = spark.read.csv("data/osoby.csv", header=True, inferSchema=True)
df_csv.printSchema()
df_csv.select("meno", "vek").show()
```

---

## âœ… Zhrnutie

- **RDD** poskytuje nÃ­zkoÃºrovÅˆovÃº kontrolu nad dÃ¡tami, vhodnÃ© na zloÅ¾itÃ© transformÃ¡cie.
- **DataFrame** poskytuje vyÅ¡Å¡Ã­ vÃ½kon, ÄitateÄ¾nosÅ¥ a podporu SQL.
- V modernÃ½ch aplikÃ¡ciÃ¡ch sa odporÃºÄa pouÅ¾Ã­vaÅ¥ **DataFrame API**, ak nie je potrebnÃ© nieÄo Å¡pecifickÃ© z RDD.

---

<a name="spark-sql"></a>

# ğŸ§  3. Spark SQL a dopyty nad dÃ¡tami

Spark SQL je modul Apache Spark, ktorÃ½ umoÅ¾Åˆuje spracovanie Å¡truktÃºrovanÃ½ch dÃ¡t pomocou SQL syntaxe alebo DataFrame API. Kombinuje vÃ½konnosÅ¥ Spark enginu s jednoduchosÅ¥ou SQL.

---

## ğŸ“‹ ÄŒo je Spark SQL?

Spark SQL umoÅ¾Åˆuje:

- vykonÃ¡vaÅ¥ SQL dopyty priamo nad veÄ¾kÃ½mi dÃ¡tami,
- manipulovaÅ¥ so Å¡truktÃºrovanÃ½mi dÃ¡tami pomocou DataFrame API,
- pracovaÅ¥ s rÃ´znymi zdrojmi dÃ¡t ako CSV, Parquet, Hive, JDBC.

---

## ğŸ”§ Vytvorenie DataFrame tabuÄ¾ky

### ğŸ§ª PrÃ­klad: NaÄÃ­tanie CSV a registrÃ¡cia ako tabuÄ¾ka

```python
df = spark.read.option("header", "true").csv("data/objednavky.csv")
df.createOrReplaceTempView("objednavky")
```

Po registrÃ¡cii mÃ´Å¾ete nad `objednavky` spÃºÅ¡Å¥aÅ¥ SQL dopyty.

---

## ğŸ§ª PrÃ­klady SQL dopytov

```python
# VÃ½ber vÅ¡etkÃ½ch stÄºpcov
spark.sql("SELECT * FROM objednavky").show()

# Filtrovanie podÄ¾a hodnoty
spark.sql("SELECT * FROM objednavky WHERE cena > 100").show()

# SkupinovÃ© vÃ½poÄty
spark.sql("SELECT produkt, COUNT(*) AS pocet FROM objednavky GROUP BY produkt").show()
```

---

## ğŸ“Š Porovnanie: SQL vs. DataFrame API

| OperÃ¡cia                     | SQL syntax                                                    | DataFrame API                                 |
|------------------------------|----------------------------------------------------------------|-----------------------------------------------|
| VÃ½ber                        | `SELECT meno FROM zakaznici`                                  | `df.select("meno")`                           |
| Filtrovanie                  | `SELECT * FROM objednavky WHERE cena > 100`                   | `df.filter(df.cena > 100)`                    |
| AgregÃ¡cia                    | `SELECT AVG(cena) FROM objednavky`                            | `df.agg({"cena": "avg"})`                     |
| Zoskupenie                   | `SELECT produkt, COUNT(*) FROM objednavky GROUP BY produkt`   | `df.groupBy("produkt").count()`              |
| Triedenie                    | `SELECT * FROM objednavky ORDER BY datum DESC`                | `df.orderBy("datum", ascending=False)`        |

---

## ğŸ—ƒï¸ PrÃ¡ca so Å¡truktÃºrovanÃ½mi formÃ¡tmi

### CSV

```python
df = spark.read.option("header", True).csv("data/objednavky.csv")
```

### JSON

```python
df_json = spark.read.json("data/produkty.json")
```

### Parquet

```python
df_parquet = spark.read.parquet("data/transakcie.parquet")
```

---

## ğŸ§  OptimalizÃ¡cia Spark SQL

- **Catalyst Optimizer** â€“ analyzuje a optimalizuje logickÃ½ plÃ¡n dopytu.
- **Tungsten Execution Engine** â€“ nÃ­zkoÃºrovÅˆovÃ¡ optimalizÃ¡cia vÃ½poÄtov.
- **Predicate Pushdown** â€“ filtruje dÃ¡ta uÅ¾ pri ich naÄÃ­tavanÃ­.

â¡ï¸ Tieto mechanizmy vÃ½razne zvyÅ¡ujÃº vÃ½kon pri spracovanÃ­ veÄ¾kÃ½ch dÃ¡t.

---

## ğŸ§ª PokroÄilÃ© SQL: JOIN, funkcie, CASE

```sql
-- Join dvoch tabuliek
SELECT o.id, o.produkt, z.meno
FROM objednavky o
JOIN zakaznici z ON o.zakaznik_id = z.id

-- PrÃ­padovÃ¡ logika
SELECT meno,
       CASE WHEN vek >= 18 THEN 'DospelÃ½' ELSE 'DieÅ¥a' END AS typ
FROM osoby
```

---

## âœ… Zhrnutie

- Spark SQL umoÅ¾Åˆuje prÃ­stup k dÃ¡tam pomocou znÃ¡mej SQL syntaxe.
- Podporuje integrÃ¡ciu s rÃ´znymi dÃ¡tovÃ½mi formÃ¡tmi (CSV, JSON, Parquet).
- VÃ½kon zabezpeÄuje Catalyst a Tungsten optimalizÃ¡cia.
- SQL dopyty sÃº Äasto kombinovanÃ© s DataFrame API v praxi.

---

